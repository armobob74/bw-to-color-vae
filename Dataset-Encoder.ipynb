{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37962021",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos_paths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path_to_data\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n",
      "File \u001b[0;32m~/Projects/bw-to-color-vae/venv/lib/python3.8/site-packages/torch/__init__.py:229\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    228\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from os_paths import path_to_data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from skimage import color\n",
    "from lab_utils import *\n",
    "import time\n",
    "from models import LVAE, ABVAE\n",
    "from os_paths import lvae_state_dict_path, abvae_state_dict_path, encodings_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91647cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FfhqDatasetAB(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, colorchannels='ab', cacheing=False, cachelim=30000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = os.listdir(root_dir)\n",
    "        #a dictionary that prevents us from doing more compute than is necessary\n",
    "        # preprocessing a batch takes about 5x as long as actually training it\n",
    "        self.cacheing = cacheing\n",
    "        self.cachelim = cachelim\n",
    "        self.already_seen_images = {}\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        # doing this is about 1000x faster than loading an image\n",
    "        if self.cacheing and image_path in self.already_seen_images:\n",
    "            return self.already_seen_images[image_path]\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = np.array(image) #go from PIL image to nparray so that rgb2lab will work\n",
    "        image = color.rgb2lab(image).astype(np.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # scale so that values are between 0 and 1\n",
    "        image = scale_lab(image)\n",
    "        image = image[1:] ### isolate ab layers\n",
    "\n",
    "        if self.cacheing == True and len(self.already_seen_images) < self.cachelim:\n",
    "            self.already_seen_images[image_path] = image\n",
    "        return image\n",
    "    \n",
    "    def preshow_image(self,image):\n",
    "        \"\"\"\n",
    "        input: torch.tensor in scaled CIELAB color space wiothout L channel, dims = (2, H, W)\n",
    "        output: np array in RGB color space \n",
    "        \"\"\"\n",
    "        empty_L = torch.zeros_like(image[0:1]) + 0.5\n",
    "        image = torch.cat((empty_L,image),dim=0)\n",
    "        image = image.numpy()\n",
    "        image = descale_lab(image) \n",
    "        image = np.moveaxis(image, 0,-1) # Convert from (C, H, W) to (H, W, C) so imshow works\n",
    "        image = color.lab2rgb(image, channel_axis=-1)\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "    def show_grid(self,nrows, ncols):\n",
    "        n = nrows * ncols\n",
    "        images = [self[i] for i in range(n)]\n",
    "        fig, axes = plt.subplots(nrows, ncols)\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < len(images):\n",
    "                image = self.preshow_image(images[i])\n",
    "                ax.imshow(image)\n",
    "                ax.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "class FfhqDatasetL(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, colorchannels='ab', cacheing=True, cachelim=50000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = os.listdir(root_dir)\n",
    "        #a dictionary that prevents us from doing more compute than is necessary\n",
    "        # preprocessing a batch takes about 5x as long as actually training it\n",
    "        self.cacheing = cacheing\n",
    "        self.cachelim = cachelim\n",
    "        self.already_seen_images = {}\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        # doing this is about 1000x faster than loading an image\n",
    "        if self.cacheing and image_path in self.already_seen_images:\n",
    "            return self.already_seen_images[image_path]\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = np.array(image) #go from PIL image to nparray so that rgb2lab will work\n",
    "        image = color.rgb2lab(image).astype(np.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # scale so that values are between 0 and 1\n",
    "        image = scale_lab(image)\n",
    "        image = image[0:1] ### isolate L layer\n",
    "\n",
    "        if self.cacheing == True and len(self.already_seen_images) < self.cachelim:\n",
    "            self.already_seen_images[image_path] = image\n",
    "        return image\n",
    "    \n",
    "    def preshow_image(self,image):\n",
    "        \"\"\"\n",
    "        input: torch.tensor in scaled CIELAB color space wiothout ab channels, dims = (1, H, W)\n",
    "        output: np array in RGB color space \n",
    "        \"\"\"\n",
    "        image = image.numpy()\n",
    "        image = np.moveaxis(image, 0,-1) # Convert from (C, H, W) to (H, W, C) so imshow works\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "    def show_grid(self,nrows, ncols):\n",
    "        n = nrows * ncols\n",
    "        images = [self[i] for i in range(n)]\n",
    "        fig, axes = plt.subplots(nrows, ncols)\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < len(images):\n",
    "                image = self.preshow_image(images[i])\n",
    "                ax.imshow(image, cmap='gray')\n",
    "                ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128),antialias=True),\n",
    "    #transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "path_to_trn = os.path.join(path_to_data,'training')\n",
    "path_to_tst = os.path.join(path_to_data,'test')\n",
    "dataset_ab = FfhqDatasetAB(root_dir=path_to_trn, transform=transform, cacheing=False)\n",
    "tst_dataset_ab = FfhqDatasetAB(root_dir=path_to_tst, transform=transform)\n",
    "\n",
    "dataset_L = FfhqDatasetL(root_dir=path_to_trn, transform=transform, cacheing=False)\n",
    "tst_dataset_L = FfhqDatasetL(root_dir=path_to_tst, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_L.show_grid(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ab.show_grid(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filepath(filepath):\n",
    "    latent_dim = filepath.split('latent_dim')[-1].split('_')[1]\n",
    "    return {'latent_dim': int(latent_dim)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6783d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvar = parse_filepath(lvae_state_dict_path)\n",
    "abvar = parse_filepath(abvae_state_dict_path)\n",
    "if abvar['latent_dim'] != lvar['latent_dim']:\n",
    "    raise ValueError(\"Latent dims must match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvae = LVAE(lvar['latent_dim']).to('cuda')\n",
    "abvae = ABVAE(abvar['latent_dim']).to('cuda')\n",
    "\n",
    "lvae.load_state_dict(torch.load(lvae_state_dict_path))\n",
    "abvae.load_state_dict(torch.load(abvae_state_dict_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68790982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset, model,printinterval=2000,device='cuda'):\n",
    "    t0 = time.time()\n",
    "    encodings = torch.Tensor([]).to(device)\n",
    "    i = 0\n",
    "    for img in dataset:\n",
    "        t1 = time.time()\n",
    "        with torch.no_grad():\n",
    "            img = img.to(device)\n",
    "            encoding = model.encode(img)\n",
    "            encodings = torch.cat((encodings,encoding))\n",
    "        if i % printinterval == 0:\n",
    "            print(f\"loop {i:5}/{len(dataset_ab)}, loop time: {time.time()-t1: 5f} sec\")\n",
    "        i +=1 \n",
    "    print(\"Time taken for encoding:\", time.time()-t0,\"seconds\")\n",
    "    print(\"Shape:\",encodings.shape)\n",
    "    return encodings.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93892314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode the ab dataset:\n",
    "abencodings = encode_dataset(dataset_ab, abvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af20926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encode the L dataset:\n",
    "Lencodings = encode_dataset(dataset_L, lvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_dir_path = './encoded_data'\n",
    "torch.save(abencodings, os.path.join(encodings_dir_path, 'ab_encodings.pt'))\n",
    "torch.save(Lencodings, os.path.join(encodings_dir_path, 'L_encodings.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
