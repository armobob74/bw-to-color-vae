{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37962021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from os_paths import path_to_data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from skimage import color\n",
    "from lab_utils import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e0950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fella is like color-VAE, but only operates on the 'L' channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\",device)\n",
    "latent_dim = 64\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91647cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FfhqDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, colorchannels='ab', cacheing=True, cachelim=50000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = os.listdir(root_dir)[0:50000]\n",
    "        #a dictionary that prevents us from doing more compute than is necessary\n",
    "        # preprocessing a batch takes about 5x as long as actually training it\n",
    "        self.cacheing = cacheing\n",
    "        self.cachelim = cachelim\n",
    "        self.already_seen_images = {}\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        # doing this is about 1000x faster than loading an image\n",
    "        if self.cacheing and image_path in self.already_seen_images:\n",
    "            return self.already_seen_images[image_path]\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = np.array(image) #go from PIL image to nparray so that rgb2lab will work\n",
    "        image = color.rgb2lab(image).astype(np.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # scale so that values are between 0 and 1\n",
    "        image = scale_lab(image)\n",
    "        image = image[0:1] ### isolate L layer\n",
    "\n",
    "        if self.cacheing == True and len(self.already_seen_images) < self.cachelim:\n",
    "            self.already_seen_images[image_path] = image\n",
    "        return image\n",
    "    \n",
    "    def preshow_image(self,image):\n",
    "        \"\"\"\n",
    "        input: torch.tensor in scaled CIELAB color space wiothout ab channels, dims = (1, H, W)\n",
    "        output: np array in RGB color space \n",
    "        \"\"\"\n",
    "        image = image.numpy()\n",
    "        image = np.moveaxis(image, 0,-1) # Convert from (C, H, W) to (H, W, C) so imshow works\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "    def show_grid(self,nrows, ncols):\n",
    "        n = nrows * ncols\n",
    "        images = [self[i] for i in range(n)]\n",
    "        fig, axes = plt.subplots(nrows, ncols)\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < len(images):\n",
    "                image = self.preshow_image(images[i])\n",
    "                ax.imshow(image, cmap='gray')\n",
    "                ax.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def preprocess_everything(self):\n",
    "        \"\"\"\n",
    "        preprocess everything up front\n",
    "        images load about 1000x faster when preprocessed\n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        n = len(self)\n",
    "        print(f\"Preprocessing {n} images\")\n",
    "        update_block = n // 100\n",
    "        percent_complete = 0\n",
    "        for i in range(n):\n",
    "            if i % update_block == 0:\n",
    "                percent_complete += 1\n",
    "                print(f\"{percent_complete}% complete\")\n",
    "            self[i]\n",
    "        print(time.time() - t0,'seconds')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128),antialias=True),\n",
    "    #transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "path_to_trn = os.path.join(path_to_data,'training')\n",
    "path_to_tst = os.path.join(path_to_data,'test')\n",
    "dataset = FfhqDataset(root_dir=path_to_trn, transform=transform, cacheing=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "tst_dataset = FfhqDataset(root_dir=path_to_tst, transform=transform)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aadedb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.show_grid(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor_as_image(image):\n",
    "        image = dataset.preshow_image(image)\n",
    "        pil_image = Image.fromarray(image[:,:,0], mode='L')\n",
    "        display(pil_image)\n",
    "        return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(model,idx=0,dataset=dataset):\n",
    "    model.eval()\n",
    "    example_batch = torch.Tensor.expand(dataset[idx],1,1,128,128).to(device)\n",
    "    with torch.no_grad():\n",
    "        example_output = model.forward(example_batch)\n",
    "    ret = {\n",
    "        'original':display_tensor_as_image(dataset[idx]),\n",
    "        'recon':display_tensor_as_image(example_output[0][0].to('cpu'))\n",
    "    }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_from_vae(model,device=device,latent_dim=latent_dim,num_samples=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        samples = model.decoder(z).cpu()\n",
    "    sample_images = [display_tensor_as_image(sample) for sample in samples]\n",
    "    return sample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e705b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar, criterion,beta=beta):\n",
    "    BCE = criterion(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e665f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LVAE\n",
    "cvae = LVAE(latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0274d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tst_vae_loss(model,criterion,dataloader=tst_dataloader):\n",
    "    tst_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            batch_loss = vae_loss(recon_batch, data, mu, logvar, criterion)   \n",
    "            tst_loss += batch_loss\n",
    "    return tst_loss / len(tst_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21aa97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "num_epochs=300\n",
    "num_epochs_completed = 0\n",
    "\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85ee6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time to train our VAE!\n",
    "\n",
    "cvae.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = cvae(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar, criterion)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    test_loss = eval_tst_vae_loss(cvae,criterion)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(dataloader.dataset):.4f}, Test loss: {test_loss:.4f}\")\n",
    "    print(\"Trn Recon example:\")\n",
    "    show_example(cvae)\n",
    "    print(\"Tst Recon example:\")\n",
    "    show_example(cvae, dataset=tst_dataset)\n",
    "    print(\"Generated example:\")\n",
    "    generate_samples_from_vae(cvae,num_samples=1)\n",
    "    cvae.train() # because the sample/example functions go into cvae.eval()\n",
    "    num_epochs_completed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tst_vae_loss(cvae,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9125aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "test_loss = eval_tst_vae_loss(cvae,criterion)\n",
    "savedir = f\"latent_dim_{latent_dim}_tst_loss_{test_loss:.4f}_epochs_{num_epochs_completed}_beta_{beta}\"\n",
    "path_to_savedirs = os.path.join('.','saved_models','LVAE')\n",
    "savedirpath = os.path.join(path_to_savedirs,savedir)\n",
    "if savedir not in os.listdir(path_to_savedirs):\n",
    "    os.mkdir(savedirpath)\n",
    "savepath = os.path.join(savedirpath,\"state_dict.pth\")\n",
    "torch.save(cvae.state_dict(), savepath)\n",
    "\n",
    "# save some example images\n",
    "num_examples_to_save = 8\n",
    "print(\"Trn Recon example(s):\")\n",
    "trn_recon_examples = [show_example(cvae,idx = n,dataset=dataset) for n in range(num_examples_to_save)]\n",
    "print(\"Tst Recon example(s):\")\n",
    "tst_recon_examples = [show_example(cvae,idx = n, dataset=tst_dataset) for n in range(num_examples_to_save)]\n",
    "print(\"Generated example(s):\")\n",
    "generated_examples = generate_samples_from_vae(cvae,num_samples=num_examples_to_save)\n",
    "\n",
    "reconpath = os.path.join(savedirpath,'recon')\n",
    "if not 'recon' in os.listdir(savedirpath):\n",
    "    os.mkdir(reconpath)\n",
    "if not 'tst' in os.listdir(reconpath):\n",
    "    os.mkdir(os.path.join(reconpath,'tst'))\n",
    "    \n",
    "if not 'trn' in os.listdir(reconpath):\n",
    "    os.mkdir(os.path.join(reconpath,'trn'))\n",
    "    \n",
    "if not 'generated' in os.listdir(savedirpath):\n",
    "    os.mkdir(os.path.join(savedirpath,'generated'))\n",
    "for i in range(num_examples_to_save):\n",
    "    trn_recon_examples[i]['original'].save(os.path.join(reconpath, 'trn', f\"example_{i}_original.png\"))\n",
    "    trn_recon_examples[i]['recon'].save(os.path.join(reconpath, 'trn', f\"example_{i}_recon.png\"))\n",
    "    tst_recon_examples[i]['original'].save(os.path.join(reconpath, 'tst', f\"example_{i}_original.png\"))\n",
    "    tst_recon_examples[i]['recon'].save(os.path.join(reconpath, 'tst', f\"example_{i}_recon.png\"))\n",
    "    generated_examples[i].save(os.path.join(savedirpath, 'generated', f\"example_{i}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6046bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_samples_from_vae(cvae,num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d4323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
